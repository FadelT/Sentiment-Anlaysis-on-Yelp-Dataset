# -*- coding: utf-8 -*-
"""Copie de Projet_Yelp_KASSINGA-Nbouyaa_PARDON-Gaetan.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VoICrAJUd6addT-0HlHV1NcnK6ErxEfE
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget https://www-eu.apache.org/dist/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz && tar xf spark-3.0.1-bin-hadoop2.7.tgz
!pip install pyspark findspark

from google.colab import drive
drive.mount('/content/drive')

!apt-get install -q libgeos-3.5.0
!apt-get install -q libgeos-dev
!pip install -q https://github.com/matplotlib/basemap/archive/master.zip

!pip install -q pyproj==1.9.6

# Commented out IPython magic to ensure Python compatibility.
from mpl_toolkits.basemap import Basemap
import matplotlib.pyplot as plt
# %matplotlib inline

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.0.1-bin-hadoop2.7"
!update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java
!java -version

import findspark
findspark.init()
from pyspark.sql import SparkSession
from pyspark import SparkContext
from pyspark.sql import SQLContext
import folium
from folium import Choropleth, Circle, Marker
from folium.plugins import HeatMap, MarkerCluster
import pandas as pd
import pyspark.sql.functions as F
from pyspark.sql import Row
import matplotlib.pyplot as plt
import seaborn as sns
from pyspark.ml import Pipeline
from pyspark.ml.classification import NaiveBayes
from pyspark.ml.feature import CountVectorizer, StringIndexer, VectorAssembler, HashingTF
from pyspark.sql.types import StructType, StringType
from pyspark.sql.functions import col, split
from pyspark.ml.feature import *
import string
import re
from pyspark.sql.functions import udf
from pyspark.mllib.classification import SVMWithSGD

spark = SparkSession.builder.appName("Projet YelpD").master("local[*]").getOrCreate()

sc = spark.sparkContext
sqlc=SQLContext(sc)

!wget -O yelp-dataset.tgz "$(wget --quiet https://www.yelp.com/dataset/download \
--load-cookies /tmp/cookies.txt \
--post-data "name=lsp&email=lsp%40lsp.lsp&signature=lsp&terms_accepted=y&csrftok=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies https://www.yelp.com/dataset/download -O- | grep -Po 'value="\K[^"]*' -m 1)" \
-O- | grep -Po 'href="\K[^"]*yelp-dataset[^"]*' -m 1 | sed s/amp\;//g)"

import tarfile
filename = "yelp-dataset.tgz"
tf = tarfile.open(filename)
print(tf.getnames())

import tarfile
filename = "yelp-dataset.tgz"
tf = tarfile.open(filename)
######Chemin à modifier#####
tf.extractall('/content/drive/My Drive/Yelp_Data/Json_Files')

df = sqlc.read.json("/content/drive/My Drive/yelp_academic_dataset_user.json")
df.write.parquet("path/to/parquet/file")

#############Chemin à modifier########
df = sqlc.read.json("/content/drive/My Drive/Yelp_Data/Json_Files/yelp_academic_dataset_user.json")
df.write.parquet("/content/drive/My Drive")

df.printSchema()
df.show()

"""Liste des documents"""

g=["yelp_academic_dataset_user","yelp_academic_dataset_tip","yelp_academic_dataset_checkin","yelp_academic_dataset_business","yelp_academic_dataset_review"]
print(len(g))

"""#Conversion des documents en format parquet"""

def to_parquet( g):
  for j in range(len(g)):
      df = sqlc.read.json("/content/drive/My Drive/Yelp_Data/Json_Files/{}.json".format(g[j]))
      #df.saveAsParquetFile("/content/drive/My Drive/Yelp_Data/Parquet_Files","{}".format(g[j]))
      df.write.parquet("/content/drive/My Drive/Yelp_Data/Parquet_Files/{}.parquet".format(g[j]))

to_parquet (g)

"""#Chargement des documents (format Parquet)

Chargement de Business
"""

dfBusiness = spark.read.parquet("/content/drive/My Drive/Yelp_Data/Parquet_Files/yelp_academic_dataset_business.parquet")
dfBusiness.show()
dfBusiness.describe()

"""Conversion en pandas pour utilisations ultérieures"""

dfBusinessPand=dfBusiness.toPandas()
dfBusinessPand.head()

"""Chargement de Review"""

dfReview=spark.read.parquet("/content/drive/My Drive/Yelp_Data/Parquet_Files/yelp_academic_dataset_review.parquet")
dfReview.show()

"""Chargement de User"""

dfUser=spark.read.parquet("/content/drive/My Drive/Yelp_Data/Parquet_Files/yelp_academic_dataset_user.parquet")
dfUser.show()

"""Quelques tests

Exemple: Extraction des données sur la cité "Las Vegas"
"""

Las_vega=dfBusiness.filter(dfBusiness.city=="Las Vegas")
Las_vega.show()
Las_vega.count()
LV1=Las_vega.first()
print(LV1.latitude, LV1.longitude)

"""#Transformation des données Business

Ici, nous allons transformer nos données Business afin d'avoir des informations interessantes.

Pour cela, nous allons compter le nombre de villes par état réferencé par Yelp correspondant ainsi au nombre de business identifiés, calculer la moyenne des avis par pays, le nombre de commentaires par pays nous donnant une idées sur la quantité d'utilisateurs. 

"""

dfBusiness_sum=dfBusiness.groupBy('state').agg({'state':"count",'latitude':"mean",'longitude':"mean",'city':"count",'review_count':"sum",'stars': "mean"})
dfBusiness_sum.show(37)

"""#Classement des pays en fonction du nombre  de commentaires des utilisateurs"""

dfBusiness.groupBy('state')\
    .agg({'state':"count",'latitude':"mean",'longitude':"mean",'city':"count",'review_count':"sum",'stars': "mean"})\
    .orderBy('sum(review_count)', ascending=False)\
    .show()

"""#Classement des pays en fonction de la moyenne des avis

Nous remarquons que les pays ayant une moyenne d'avis supérieur à 4 ont généralement moins  d'avis  que les autres pays.
"""

dfBusiness.groupBy('state')\
    .agg({'state':"count",'latitude':"mean",'longitude':"mean",'city':"count",'review_count':"sum",'stars': "mean"})\
    .orderBy('avg(stars)', ascending=False)\
    .show()

"""#Conversion de nos dataframes en pandas pour la visualisation"""

dfBusiness_sumPand=dfBusiness_sum.toPandas()
dfBusiness_sumPand.info()
len(dfBusiness_sumPand)

"""#World view

Vue globale pour voir d'où viennent les données de Yelp.

Les données viennent principalement des Etats Unis et du Canada.
"""

#basic basemap of the world
plt.figure(1, figsize=(15,6))
# use ortho projection for the globe type version
m1=Basemap(projection='ortho',lat_0=20,lon_0=-50)

# hex codes from google maps color pallete = http://www.color-hex.com/color-palette/9261
#add continents
m1.fillcontinents(color='#bbdaa4',lake_color='#4a80f5') 
# add the oceans
m1.drawmapboundary(fill_color='#4a80f5')                
# Draw the boundaries of the countires
m1.drawcountries(linewidth=0.1, color="black")

#Add the scatter points to indicate the locations of the businesses
mxy = m1(dfBusinessPand["longitude"].tolist(), dfBusinessPand["latitude"].tolist())
m1.scatter(mxy[0], mxy[1], s=3, c="orange", lw=3, alpha=1, zorder=5)
plt.title("World-wide Yelp Reviews")
plt.show()

"""#Les Etats référencés par Yelp et Fréquence des établissements par Etat

Plus la couleur tend vers le rouge, plus il y'a beaucoup d'établissements dans cet état qui sont réferencé dans l'état
"""

m_1 = folium.Map(location=[37.2616057457,-77.3254377349], tiles='cartodbpositron', zoom_start=10)
for idx, row in dfBusiness_sumPand.iterrows():
    Marker([row['avg(latitude)'], row['avg(longitude)']], popup='{}'.format(dfBusiness_sumPand.iloc[idx]['state'])).add_to(m_1)


dfBusinessPans_mat=dfBusinessPand[['latitude','longitude']].values

HeatMap(dfBusinessPans_mat, radius=15).add_to(m_1)

m_1

dfBusinessPand[['latitude','longitude']].info()

"""#  Repésentation de la Moyenne d'avis  par pays en trois classes:
# { Vert: [1,2], Rouge: ]2,4[, Bleu: [4,5]}


"""

m_2 = folium.Map(location=[37.2616057457,-77.3254377349], tiles='cartodbpositron', zoom_start=6)

def color_producer(val):
    if val <= 2:
        return 'forestgreen'
    elif 2<val<4:
        return 'darkred'
    else:
        return 'blue'

# Add a bubble map to the base map
for i in range(0,len(dfBusiness_sumPand)):
    Circle(
        location=[dfBusiness_sumPand.iloc[i]['avg(latitude)'], dfBusiness_sumPand.iloc[i]['avg(longitude)']],
        radius=200,
        color=color_producer(dfBusiness_sumPand.iloc[i]['avg(stars)'])).add_to(m_2)

# Display the map
m_2

"""Ici, nous réalisons que la moyenne des avis par pays se situe généralement entre 2 et 4(les bubble rouge).

#Les citées les plus visitées
"""

color = sns.color_palette()
x=dfBusinessPand['city'].value_counts()
x=x.sort_values(ascending=False)
x=x.iloc[0:20]
plt.figure(figsize=(16,4))
ax = sns.barplot(x.index, x.values, alpha=0.8, color=color[2])
plt.title(" Quelle est la cité la plus commentée?")
locs, labels = plt.xticks()
plt.setp(labels, rotation=45)
plt.ylabel('# businesses', fontsize=12)
plt.xlabel('City', fontsize=12)

#adding the text labels
rects = ax.patches
labels = x.values
for rect, label in zip(rects, labels):
    height = rect.get_height()
    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')

plt.show()

locations = dfBusiness.select('business_id','city')
review_city = dfReview.select('business_id')
merge_city = locations.join(review_city,'business_id','inner')
grouped_review_city = merge_city.groupby('city').count()
most_reviewed_city = grouped_review_city.groupby('city').sum()
most_reviewed_city.sort('sum(count)',ascending=False).show(10)

"""Ici, nous avons "Las Vegas" en tête suivi de "Phoenix", "Toronto", "Scottsdale".

#Les catégories de business les plus populaires
"""

dfBusinessPand.dropna(
    axis=0,
    how='any',
    thresh=None,
    subset=None,
    inplace=True
)

business_cats=' '.join(dfBusinessPand['categories'])

cats=pd.DataFrame(business_cats.split(';'),columns=['category'])
x=cats.category.value_counts()
print("There are ",len(x)," different types/categories of Businesses in Yelp!")
#prep for chart
x=x.sort_values(ascending=False)
x=x.iloc[0:20]

#chart
plt.figure(figsize=((16,16)))
ax = sns.barplot(x.index, x.values, alpha=0.8)#,color=color[5])
plt.title("What are the top categories?",fontsize=25)
locs, labels = plt.xticks()
plt.setp(labels, rotation=80)
plt.ylabel('# businesses', fontsize=12)
plt.xlabel('Category', fontsize=12)

#adding the text labels
rects = ax.patches
labels = x.values
for rect, label in zip(rects, labels):
    height = rect.get_height()
    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')

plt.show()

"""  Il y'a un soucis de taille de l'image, par conséquent nous allons représenter ces données dans un tableau en utilisant spark"""

from pyspark.sql.functions import split,explode
category = dfBusiness.select('categories')
individual_category = category.select(explode(split('categories', ',')).alias('category'))
grouped_category = individual_category.groupby('category').count()
top_category = grouped_category.sort('count',ascending=False)
top_category.show(15,truncate=False)

"""Donc nous voyons que la catégorie la plus populaire est les restaurants  suivie du shopping.

#Distribution de la répartition  des avis
"""

x=dfBusinessPand['stars'].value_counts()
x=x.sort_index()
#plot
plt.figure(figsize=(8,4))
ax= sns.barplot(x.index, x.values, alpha=0.8)
plt.title("Star Rating Distribution")
plt.ylabel('# of businesses', fontsize=12)
plt.xlabel('Star Ratings ', fontsize=12)

#adding the text labels
rects = ax.patches
labels = x.values
for rect, label in zip(rects, labels):
    height = rect.get_height()
    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')

plt.show()

rating = dfBusiness.select('stars')
group_rating = rating.groupby('stars').count()
rating_top = group_rating.sort('count',ascending=False)
rating_top.show(truncate=False)

#get all ratings data
rating_data=dfBusinessPand[['latitude','longitude','stars','review_count']]
# Creating a custom column popularity using stars*no_of_reviews
rating_data['popularity']=rating_data['stars']*rating_data['review_count']
f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,7))

#a random point inside vegas
lat = 36.207430
lon = -115.268460
#some adjustments to get the right pic
lon_min, lon_max = lon-0.3,lon+0.5
lat_min, lat_max = lat-0.4,lat+0.5
#subset for vegas
ratings_data_vegas=rating_data[(rating_data["longitude"]>lon_min) &\
                    (rating_data["longitude"]<lon_max) &\
                    (rating_data["latitude"]>lat_min) &\
                    (rating_data["latitude"]<lat_max)]

#Facet scatter plot
ratings_data_vegas.plot(kind='scatter', x='longitude', y='latitude',
                color='yellow', 
                s=.02, alpha=.6, subplots=True, ax=ax1)
ax1.set_title("Las Vegas")
ax1.set_facecolor('black')

#a random point inside pheonix
lat = 33.435463
lon = -112.006989
#some adjustments to get the right pic
lon_min, lon_max = lon-0.3,lon+0.5
lat_min, lat_max = lat-0.4,lat+0.5
#subset for pheonix
ratings_data_pheonix=rating_data[(rating_data["longitude"]>lon_min) &\
                    (rating_data["longitude"]<lon_max) &\
                    (rating_data["latitude"]>lat_min) &\
                    (rating_data["latitude"]<lat_max)]
#plot pheonix
ratings_data_pheonix.plot(kind='scatter', x='longitude', y='latitude',
                color='yellow', 
                s=.02, alpha=.6, subplots=True, ax=ax2)
ax2.set_title("Pheonix")
ax2.set_facecolor('black')
f.show()

"""Il est intéressant de noter que la structure en blocs ou en grille des villes américaines est différente de celle des autres villes.

#Analyse des données Users
"""

#=dfUser.toPandas()

user_agg[('review_id','count')].loc[user_agg[('review_id','count')]>30] = 30
plt.figure(figsize=(12,5))
plt.suptitle("User Deep dive",fontsize=20)
gridspec.GridSpec(1,2)
plt.subplot2grid((1,2),(0,0))
#Cumulative Distribution
ax=sns.kdeplot(user_agg[('review_id','count')],shade=True,color='r')
plt.title("How many reviews does an average user give?",fontsize=15)
plt.xlabel('# of reviews given', fontsize=12)
plt.ylabel('# of users', fontsize=12)

#Cumulative Distribution
plt.subplot2grid((1,2),(0,1))
sns.distplot(user_agg[('review_id','count')],
             kde_kws=dict(cumulative=True))
plt.title("Cumulative dist. of user reviews",fontsize=15)
plt.ylabel('Cumulative perc. of users', fontsize=12)
plt.xlabel('# of reviews given', fontsize=12)

plt.show()
end_time=time.time()
print("Took",end_time-start_time,"s")

"""#Partie Machine Learning: Sentiment analysis

Nous allons nettoyer nos données reviews en enlevant la ponctuation.
"""

def remove_punct(text):
    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\r\\t\\n]')
    nopunct = regex.sub(" ", text)  
    return nopunct

"""Nous définissons notre seuil qui va nous permettre de distinguer un commentaire positif d'un commentaire négatif. Un commentaire est positif si le rating est suéprieur ou égal à 3 et négatif sinon. Nous définissons cela à partir de notre fonction convert_rating."""

def convert_rating(rating):
    if rating >=3:
        return 1
    else:
        return 0

from pyspark.sql.functions import udf
punct_remover = udf(lambda x: remove_punct(x))
rating_convert = udf(lambda x: convert_rating(x))

#select 1.5 mn rows of reviews text and corresponding star rating with punc removed and ratings converted
resultDF = dfReview.select('review_id', punct_remover('text'), rating_convert('stars')).limit(1500000)

#user defined functions change column names so we rename the columns back to its original names
resultDF = resultDF.withColumnRenamed('<lambda>(text)', 'text')
resultDF = resultDF.withColumnRenamed('<lambda>(stars)', 'stars')

resultDF.show()

train, test = resultDF.randomSplit([0.9, 0.1], seed=12345)

from pyspark.sql.types import StructType, StringType
from pyspark.sql.functions import col, split

schema = StructType().add('stars', StringType()) \
  .add('text', StringType()) 

def vectorize(df):
  return df.withColumn('text', split(col('text'), "\s"))

#train1 = spark.read.csv("train.csv", header=False, schema=schema)
train = vectorize(train)
test = vectorize(test)

"""#Construction d'un Pipeline avec le modèle Naive Bayes"""

from pyspark.ml.classification import NaiveBayes
from pyspark.ml import Pipeline
from pyspark.ml.classification import NaiveBayes
from pyspark.ml.feature import CountVectorizer, StringIndexer, VectorAssembler, HashingTF
from pyspark.sql.types import StructType, StringType
from pyspark.sql.functions import col, split

vectorizer1 = CountVectorizer(inputCol="text", outputCol="text_encoded")
#hashingTF = HashingTF(numFeatures=2048, inputCol="article", outputCol="article_encoded")
assembler = VectorAssembler(inputCols=["text_encoded"], outputCol="features")
label_indexer = StringIndexer(inputCol="stars", outputCol="stars_index")
classifier = NaiveBayes(labelCol="stars_index", predictionCol="prediction")
pipeline = Pipeline(stages=[vectorizer1, assembler, label_indexer, classifier])
pipeline_model = pipeline.fit(train)

test_predicted = pipeline_model.transform(test)

test_predicted.show()

"""#Evaluation du modèle"""

from pyspark.ml.evaluation import MulticlassClassificationEvaluator
evaluator = MulticlassClassificationEvaluator(labelCol="stars_index",
                                              metricName="f1")
f1 = evaluator.evaluate(test_predicted)
print("F1 score = {:.2f}".format(f1))

"""Nous obtenons une performance de 88%  de précision avec le Naive Bayes comme classifieur. Ce qui est déjà bien. Il est à noter que le modèle prend environ 5 min pour l'évaluation du modèle et 24 min pour l'entraînement.

#Construction d'un autre Pipeline avec la regression Logistic
"""

numIterations = 50
regParam = 0.3

from pyspark.ml.classification import LogisticRegression


vectorizer1 = CountVectorizer(inputCol="text", outputCol="text_encoded")
#hashingTF = HashingTF(numFeatures=2048, inputCol="article", outputCol="article_encoded")
assembler = VectorAssembler(inputCols=["text_encoded"], outputCol="features")
label_indexer = StringIndexer(inputCol="stars", outputCol="stars_index")
#classifier =LogisticRegression(featuresCol='stars_index',labelCol="prediction") 
classifier=LogisticRegression(maxIter=10, regParam=0.01,predictionCol="prediction", labelCol="stars_index")
pipeline = Pipeline(stages=[vectorizer1, assembler, label_indexer, classifier])
pipeline_model = pipeline.fit(train)

test_predicted = pipeline_model.transform(test)

"""#Evaluation du modèle Regression Logistique"""

from pyspark.ml.evaluation import MulticlassClassificationEvaluator
evaluator = MulticlassClassificationEvaluator(labelCol="stars_index",
                                              metricName="f1")
f1 = evaluator.evaluate(test_predicted)
print("F1 score = {:.2f}".format(f1))